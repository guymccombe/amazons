{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "!pip install -q torch torchvision livelossplot tqdm pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from copy import copy\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from os.path import join, dirname\n",
    "import pandas as pd\n",
    "from random import randint, getrandbits\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self):\n",
    "        # suppress np divide by zero errors for moves\n",
    "        np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "        self.board = np.zeros((3, 10, 10), dtype=np.uint8)\n",
    "        self.isBlackTurn = False\n",
    "        self.__loadStartingBoard()\n",
    "\n",
    "    def __loadStartingBoard(self):\n",
    "        # White amazons\n",
    "        self.board[0, 0, 3] = 1\n",
    "        self.board[0, 3, 0] = 1\n",
    "        self.board[0, 6, 0] = 1\n",
    "        self.board[0, 9, 3] = 1\n",
    "\n",
    "        # Black amazons\n",
    "        self.board[1, 0, 6] = 1\n",
    "        self.board[1, 3, 9] = 1\n",
    "        self.board[1, 6, 9] = 1\n",
    "        self.board[1, 9, 6] = 1\n",
    "\n",
    "    def move(self, fromXY, toXY, shootAT):\n",
    "        unchangedBoard = copy(self.board)\n",
    "\n",
    "        if self.board[int(self.isBlackTurn), fromXY[0], fromXY[1]] == 1:\n",
    "            if self.__isValidMove(fromXY, toXY):\n",
    "                self.board[int(self.isBlackTurn), fromXY[0], fromXY[1]] = 0\n",
    "                self.board[int(self.isBlackTurn), toXY[0], toXY[1]] = 1\n",
    "                if self.__isValidMove(toXY, shootAT):\n",
    "                    self.board[2, shootAT[0], shootAT[1]] = 1\n",
    "                    self.isBlackTurn = not self.isBlackTurn\n",
    "                    return\n",
    "\n",
    "        # if any invalidities\n",
    "        self.board = unchangedBoard\n",
    "        raise Exception(\"That move is invalid.\")\n",
    "\n",
    "    def __isValidMove(self, position, destination):\n",
    "        direction = np.subtract(destination, position)\n",
    "        direction = direction / np.abs(direction)\n",
    "        direction = np.array(np.nan_to_num(direction), dtype=np.int8)\n",
    "\n",
    "        valid = True\n",
    "        checkingPosition = position + direction\n",
    "\n",
    "        while tuple(checkingPosition) != destination:\n",
    "            for i in range(3):\n",
    "                valid &= (\n",
    "                    self.board[i, checkingPosition[0], checkingPosition[1]] == 0)\n",
    "            checkingPosition = checkingPosition + direction\n",
    "\n",
    "        return valid\n",
    "\n",
    "    def isGameFinished(self):\n",
    "        currentPlayersAmazons = np.nonzero(self.board[int(self.isBlackTurn)])\n",
    "        currentPlayersAmazons = np.transpose(currentPlayersAmazons)\n",
    "        canMove = False\n",
    "\n",
    "        # Check if there exists an empty cell neighbouring an amazon\n",
    "        for amazon in currentPlayersAmazons:\n",
    "            for i in range(-1, 2):\n",
    "                for j in range(-1, 2):\n",
    "                    if (i, j) != (0, 0):\n",
    "                        pos = amazon[0] + i, amazon[1] + j\n",
    "                        if self.__isInBoard(pos):\n",
    "                            isPosEmpty = True\n",
    "                            for k in range(3):\n",
    "                                isPosEmpty &= self.board[k,\n",
    "                                                         pos[0], pos[1]] == 0\n",
    "                            if isPosEmpty:\n",
    "                                return False\n",
    "        return True\n",
    "\n",
    "    def __isInBoard(self, pos):\n",
    "        isIn = pos[0] > -1 and pos[0] < 10\n",
    "        isIn &= pos[1] > -1 and pos[1] < 10\n",
    "        return isIn\n",
    "\n",
    "    def calculateReward(self):\n",
    "        winner = int(not self.isBlackTurn)\n",
    "        winnersAmazons = np.nonzero(self.board[winner])\n",
    "        winnersAmazons = np.transpose(winnersAmazons)\n",
    "\n",
    "        rewardCells = np.zeros((10, 10), dtype=np.uint8)\n",
    "        for amazon in winnersAmazons:\n",
    "            rewardCells = self.__rewardHelper(amazon, rewardCells)\n",
    "\n",
    "        return(np.count_nonzero(rewardCells))\n",
    "\n",
    "    def __rewardHelper(self, start, cells):\n",
    "        for i in range(-1, 2):\n",
    "            for j in range(-1, 2):\n",
    "                if (i, j) != (0, 0):\n",
    "                    pos = start[0] + i, start[1] + j\n",
    "                    if self.__isInBoard(pos):\n",
    "                        if cells[pos[0], pos[1]] == 0:\n",
    "                            empty = True\n",
    "                            for k in range(3):\n",
    "                                empty &= self.board[k, pos[0], pos[1]] == 0\n",
    "\n",
    "                            if empty:\n",
    "                                cells[pos[0], pos[1]] = 1\n",
    "                                cells = self.__rewardHelper(pos, cells)\n",
    "        return cells\n",
    "\n",
    "    def rollbackTo(self, board, turn):\n",
    "        self.board = board\n",
    "        self.isBlackTurn = turn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self):\n",
    "        self.game = Game()\n",
    "        self.currentCheckpoint = None\n",
    "\n",
    "    def isGameFinished(self):\n",
    "        return self.game.isGameFinished()\n",
    "\n",
    "    def getState(self):\n",
    "        return tuple(self.game.board)\n",
    "\n",
    "    def isBlackTurn(self):\n",
    "        return self.game.isBlackTurn\n",
    "\n",
    "    def getReward(self):\n",
    "        return self.game.calculateReward()/50\n",
    "\n",
    "    def move(self, fromXY, toXY, shotXY):\n",
    "        return self.game.move(fromXY, toXY, shotXY)\n",
    "\n",
    "    def saveCheckpoint(self):\n",
    "        self.currentCheckpoint = {\n",
    "            \"board\": self.game.board,\n",
    "            \"turn\": self.game.isBlackTurn\n",
    "        }\n",
    "\n",
    "    def loadCheckpoint(self):\n",
    "        self.game.board = copy(self.currentCheckpoint[\"board\"])\n",
    "        self.game.isBlackTurn = copy(self.currentCheckpoint[\"turn\"])\n",
    "\n",
    "    def getSelectionMask(self):\n",
    "        return self.game.board[int(self.game.isBlackTurn)]\n",
    "\n",
    "    def getMovementMask(self, moveFrom):\n",
    "        occupiedCells = []\n",
    "        for stateImg in self.game.board:\n",
    "            occupiedCells += list(map(tuple, np.argwhere(stateImg > 0)))\n",
    "        return self.__validityPoller(moveFrom, occupiedCells)\n",
    "\n",
    "    def __validityPoller(self, start, blockers):\n",
    "        valid = np.zeros((10, 10))\n",
    "        for dirX in range(-1, 2):\n",
    "            for dirY in range(-1, 2):\n",
    "                if dirX == 0 and dirY == 0:\n",
    "                    continue    # Goes nowhere\n",
    "\n",
    "                x, y = start\n",
    "                x += dirX\n",
    "                y += dirY\n",
    "\n",
    "                while x > -1 and x < 10 and y > -1 and y < 10:\n",
    "                    if (x, y) in blockers:\n",
    "                        break\n",
    "\n",
    "                    valid[x, y] = 1\n",
    "\n",
    "                    x += dirX\n",
    "                    y += dirY\n",
    "\n",
    "        return valid\n",
    "\n",
    "    def getShotMask(self, newAmazonPos, oldAmazonPos):\n",
    "        occupiedCells = []\n",
    "        for stateImg in self.game.board:\n",
    "            occupiedCells += list(map(tuple, np.argwhere(stateImg > 0)))\n",
    "\n",
    "        if oldAmazonPos in occupiedCells:\n",
    "            occupiedCells.remove(oldAmazonPos)\n",
    "\n",
    "        return self.__validityPoller(newAmazonPos, occupiedCells)\n",
    "\n",
    "    def toString(self, state=None):\n",
    "\n",
    "        if state is None:\n",
    "            state = self.currentCheckpoint[\"board\"]\n",
    "\n",
    "        string = \"\"\n",
    "        for arr in state[:-1]:\n",
    "            for point in np.transpose(np.nonzero(arr)):\n",
    "                string += str(point[0]) + str(point[1])\n",
    "\n",
    "        for char in np.nditer(state[-1]):\n",
    "            string += str(char)\n",
    "\n",
    "        return string\n",
    "\n",
    "    def parseState(self, string):\n",
    "        amazons = []\n",
    "        for i in range(0, 17, 2):\n",
    "            amazons += [(int(string[i]), int(string[i+1]))]\n",
    "\n",
    "        ownAmazons = np.zeros((10, 10), dtype=np.uint8)\n",
    "        for amazon in amazons[:4]:\n",
    "            ownAmazons[amazon] = 1\n",
    "\n",
    "        oppAmazons = np.zeros((10, 10), dtype=np.uint8)\n",
    "        for amazon in amazons[4:]:\n",
    "            oppAmazons[amazon] = 1\n",
    "\n",
    "        arrows = np.fromstring(string[16:116], dtype=np.uint8)\n",
    "        arrows -= ord('0')  # Convert from unicode to binary\n",
    "        arrows = np.reshape(arrows, (10, 10))\n",
    "\n",
    "        selection, movement = None, None\n",
    "\n",
    "        if len(string) > 116:\n",
    "            selection = (int(string[117]), int(string[118]))\n",
    "\n",
    "        if len(string) > 118:\n",
    "            movement = (int(string[119]), int(string[120]))\n",
    "\n",
    "        return ownAmazons, oppAmazons, arrows, selection, movement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    NUMBER_OF_RESIDUAL_LAYERS = 40\n",
    "\n",
    "    def __init__(self, in_channels=3):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.inputLayer = self.__inputLayer(in_channels)\n",
    "        self.residualBlock = self.__residualBlock()\n",
    "        self.policyHead = self.__policyHead()\n",
    "        self.valueHead = self.__valueHead()\n",
    "\n",
    "    def __inputLayer(self, in_channels):\n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels=300,\n",
    "                                kernel_size=3, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(300))\n",
    "        layers.append(nn.ReLU())\n",
    "        return layers\n",
    "\n",
    "    def __residualBlock(self):\n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(nn.Conv2d(in_channels=300, out_channels=300,\n",
    "                                kernel_size=3, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(300))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Conv2d(in_channels=300, out_channels=300,\n",
    "                                kernel_size=3, padding=1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(300))\n",
    "        return layers\n",
    "\n",
    "    def __policyHead(self):\n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(nn.Conv2d(in_channels=300, out_channels=1,\n",
    "                                kernel_size=1, padding=0, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(10, 10))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        return layers\n",
    "\n",
    "    def __valueHead(self):\n",
    "        layers = nn.ModuleList()\n",
    "        layers.append(nn.Conv2d(in_channels=300, out_channels=1,\n",
    "                                kernel_size=1, padding=0, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(1))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(10, 10))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(10, 10))\n",
    "        layers.append(nn.MaxPool2d(10))\n",
    "        layers.append(nn.Tanh())\n",
    "        return layers\n",
    "\n",
    "    def forward(self, networkInput):\n",
    "        for layer in self.inputLayer:\n",
    "            networkInput = layer(networkInput)\n",
    "\n",
    "        for _ in range(self.NUMBER_OF_RESIDUAL_LAYERS):\n",
    "            original = networkInput     # For skip connection\n",
    "            for layer in self.residualBlock:\n",
    "                networkInput = layer(networkInput)\n",
    "            networkInput = torch.relu(networkInput + original)\n",
    "\n",
    "        policy = networkInput\n",
    "        value = networkInput\n",
    "        for layer in self.policyHead:\n",
    "            policy = layer(policy)\n",
    "\n",
    "        for layer in self.valueHead:\n",
    "            value = layer(value)\n",
    "\n",
    "        return policy.view(10, 10), value\n",
    "\n",
    "    def save(self, name):\n",
    "        path = join(dirname(__file__), f\"models\\\\{name}\")\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def __loadPath(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()\n",
    "\n",
    "    def load(self, name):\n",
    "        path = join(dirname(__file__), f\"models\\\\{name}\")\n",
    "        self.__loadPath(path)\n",
    "\n",
    "    def loadMostRecent(self, typeOfNet):\n",
    "        directory = join(dirname(__file__), \"models\")\n",
    "        allPaths = [join(directory, name)\n",
    "                    for name in listdir(directory) if typeOfNet in name]\n",
    "\n",
    "        if len(allPaths) < 1:\n",
    "            print(\"There are no saved models in the models folder. Starting fresh..\")\n",
    "        else:\n",
    "            self.__loadPath(max(allPaths, key=getctime))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    net = NeuralNet()\n",
    "    net.loadMostRecent(\"a\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "\n",
    "    def __init__(self, env, nets, device):\n",
    "        self.env = env\n",
    "        self.nets = nets\n",
    "        self.device = device\n",
    "\n",
    "        self.qValues = {}\n",
    "        self.policies = {}\n",
    "        self.edgeVisitQuantity = {}\n",
    "        self.nodeVisitQuantity = {}\n",
    "\n",
    "        self.valids = {}\n",
    "\n",
    "    def search(self):\n",
    "        if self.env.isGameFinished():\n",
    "            # Returns 3-tuple reward\n",
    "            return (self.env.getReward(),) * 3\n",
    "\n",
    "        values = [0, 0, 0]\n",
    "\n",
    "        state = self.env.getState()\n",
    "        stateString = self.env.toString(state)\n",
    "        stateTensor = torch.tensor(\n",
    "            state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
    "\n",
    "        if stateString not in self.policies:\n",
    "            self.policies[stateString], values[0] = self.nets[0](stateTensor)\n",
    "            validSelections = torch.tensor(\n",
    "                self.env.getSelectionMask(), device=self.device)\n",
    "\n",
    "            self.policies[stateString] *= validSelections  # mask out invalids\n",
    "\n",
    "            validCoordinates = np.nonzero(validSelections)\n",
    "            self.valids[stateString] = validCoordinates\n",
    "\n",
    "        validCoordinates = self.valids[stateString]\n",
    "        bestScore = float(\"-inf\")\n",
    "        bestSelection = None\n",
    "        bestSelectionStr = \"\"\n",
    "        bestSelectionArr = None\n",
    "\n",
    "        for selection in validCoordinates:\n",
    "            selection = tuple(coord.item() for coord in selection)\n",
    "            selectionArr = np.zeros((10, 10), dtype=np.uint8)\n",
    "            selectionArr[selection] = 1\n",
    "            selectionTuple = state + (selectionArr,)\n",
    "            selectionString = stateString + \\\n",
    "                str(selection[0]) + str(selection[1])\n",
    "\n",
    "            if selectionString not in self.valids:\n",
    "                self.valids[selectionString] = self.env.getMovementMask(\n",
    "                    selection)\n",
    "\n",
    "            if len(np.transpose(np.nonzero(self.valids[selectionString]))) < 1:\n",
    "                valid = self.valids[stateString]\n",
    "                selectionTensor = torch.tensor(\n",
    "                    selection, device=self.device, dtype=torch.float)\n",
    "\n",
    "                for i in range(len(valid)):\n",
    "                    if torch.all(torch.eq(valid[i],  selectionTensor)):\n",
    "                        valid = torch.cat([valid[:i], valid[i+1:]])\n",
    "                        break\n",
    "\n",
    "                self.valids[stateString] = valid\n",
    "\n",
    "            else:\n",
    "                if (stateString, selection) in self.qValues:\n",
    "                    score = self.qValues[(stateString, selection)] + \\\n",
    "                        self.policies[stateString][selection] * \\\n",
    "                        sqrt(self.nodeVisitQuantity.get(stateString, 0)) / \\\n",
    "                        (self.edgeVisitQuantity.get((stateString, selection), 0) + 1)\n",
    "\n",
    "                else:\n",
    "                    score = self.policies[stateString][selection] * \\\n",
    "                        sqrt(self.nodeVisitQuantity.get(stateString, 0))\n",
    "\n",
    "                if score > bestScore:\n",
    "                    bestScore = score\n",
    "                    bestSelection = selection\n",
    "                    bestSelectionStr = selectionString\n",
    "                    bestSelectionArr = selectionTuple\n",
    "\n",
    "        if bestSelectionStr not in self.policies:\n",
    "            movementTensor = torch.tensor(\n",
    "                bestSelectionArr, dtype=torch.float, device=self.device).unsqueeze(0)\n",
    "            self.policies[bestSelectionStr], values[1] = self.nets[1](\n",
    "                movementTensor)\n",
    "\n",
    "            # mask out invalids\n",
    "            self.policies[bestSelectionStr] *= torch.tensor(\n",
    "                self.valids[bestSelectionStr], device=self.device)\n",
    "\n",
    "        validCoordinates = np.transpose(\n",
    "            np.nonzero(self.valids[bestSelectionStr]))\n",
    "        bestScore = float(\"-inf\")\n",
    "        bestMove = None\n",
    "\n",
    "        # Choose coord to move to\n",
    "        for moveTo in validCoordinates:\n",
    "            moveTo = tuple(coord.item() for coord in moveTo)\n",
    "\n",
    "            if (bestSelectionStr, moveTo) in self.qValues:\n",
    "                score = self.qValues[(bestSelectionStr, moveTo)] + \\\n",
    "                    self.policies[bestSelectionStr][moveTo] * \\\n",
    "                    sqrt(self.nodeVisitQuantity.get(bestSelectionStr, 0)) / \\\n",
    "                    (self.edgeVisitQuantity.get((bestSelectionStr, moveTo), 0) + 1)\n",
    "            else:\n",
    "                score = self.policies[bestSelectionStr][moveTo] * \\\n",
    "                    sqrt(self.nodeVisitQuantity.get(bestSelectionStr, 0))\n",
    "\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestMove = moveTo\n",
    "\n",
    "        bestMoveString = bestSelectionStr + str(bestMove[0]) + str(bestMove[1])\n",
    "        shotTensor = torch.tensor(\n",
    "            bestSelectionArr, dtype=torch.float, device=self.device)\n",
    "        shotTensor[0][bestSelection], shotTensor[0][bestMove] = 0, 1\n",
    "        shotTensor[3][bestSelection], shotTensor[3][bestMove] = 0, 1\n",
    "        shotTensor.unsqueeze_(0)\n",
    "\n",
    "        # New leaf node\n",
    "        if bestMoveString not in self.policies:\n",
    "            self.policies[bestMoveString], values[2] = self.nets[2](shotTensor)\n",
    "            validSelections = torch.tensor(\n",
    "                self.env.getShotMask(bestMove, bestSelection), device=self.device)\n",
    "\n",
    "            # mask out invalids\n",
    "            self.policies[bestMoveString] *= validSelections\n",
    "\n",
    "            validCoordinates = np.nonzero(validSelections)\n",
    "            self.valids[bestMoveString] = validCoordinates\n",
    "            return [-value for value in values]\n",
    "\n",
    "        validShots = self.valids[bestMoveString]\n",
    "        bestScore = float(\"-inf\")\n",
    "        bestShot = None\n",
    "\n",
    "        for shot in validShots:\n",
    "            shot = tuple(coord.item() for coord in shot)\n",
    "            if (bestMoveString, shot) in self.qValues:\n",
    "                score = self.qValues[(bestMoveString, shot)] + \\\n",
    "                    self.policies[bestMoveString][shot] * \\\n",
    "                    sqrt(self.nodeVisitQuantity.get(bestMoveString, 0)) / \\\n",
    "                    (self.edgeVisitQuantity.get((bestMoveString, shot), 0) + 1)\n",
    "            else:\n",
    "                score = self.policies[bestMoveString][shot] * \\\n",
    "                    sqrt(self.nodeVisitQuantity.get(bestMoveString, 0))\n",
    "\n",
    "            if score > bestScore:\n",
    "                bestScore = score\n",
    "                bestShot = shot\n",
    "\n",
    "        self.env.move(bestSelection, bestMove, bestShot)\n",
    "\n",
    "        values = self.search()\n",
    "        pairs = ((stateString, bestSelection),\n",
    "                 (bestSelectionStr, bestMove),\n",
    "                 (bestMoveString, bestShot))\n",
    "\n",
    "        for i in range(3):\n",
    "            if pairs[i] in self.qValues:\n",
    "                self.qValues[pairs[i]] = (\n",
    "                    self.qValues[pairs[i]] *\n",
    "                    self.edgeVisitQuantity[pairs[i]] +\n",
    "                    values[i] /\n",
    "                    (self.edgeVisitQuantity[pairs[i]] + 1))\n",
    "\n",
    "                self.edgeVisitQuantity[pairs[i]] += 1\n",
    "\n",
    "            else:\n",
    "                x, y = pairs[i][1]\n",
    "                self.qValues[pairs[i]] = values[i]\n",
    "                self.edgeVisitQuantity[pairs[i]] = 1\n",
    "\n",
    "            if pairs[i][0] in self.nodeVisitQuantity:\n",
    "                self.nodeVisitQuantity[pairs[i][0]] += 1\n",
    "            else:\n",
    "                self.nodeVisitQuantity[pairs[i][0]] = 1\n",
    "\n",
    "        return [-value for value in values]\n",
    "\n",
    "    def getRandomMove(self):\n",
    "        selectionState = self.env.toString()\n",
    "        selection, selPolicy = self.__weightedRandomAction(selectionState)\n",
    "\n",
    "        movementState = selectionState + str(selection[0]) + str(selection[1])\n",
    "        moveTo, movePolicy = self.__weightedRandomAction(movementState)\n",
    "\n",
    "        shootAtState = movementState + str(moveTo[0]) + str(moveTo[1])\n",
    "        shootAt, shotPolicy = self.__weightedRandomAction(shootAtState)\n",
    "\n",
    "        return (selection, moveTo, shootAt), [[selectionState, selPolicy, self.env.isBlackTurn()],\n",
    "                                              [movementState, movePolicy,\n",
    "                                               self.env.isBlackTurn()],\n",
    "                                              [shootAtState, shotPolicy, self.env.isBlackTurn()]]\n",
    "\n",
    "    def __weightedRandomAction(self, state):\n",
    "        filtered = {key: value for (key, value)\n",
    "                    in self.edgeVisitQuantity.items() if key[0] == state}\n",
    "\n",
    "        total = sum(filtered.values())\n",
    "        adjusted = {key: value / total\n",
    "                    for (key, value) in filtered.items()}\n",
    "\n",
    "        randomChoice, total, action = random(), 0, None\n",
    "        for key, value in adjusted.items():\n",
    "            total += value\n",
    "            if randomChoice <= total:\n",
    "                action = key\n",
    "                break\n",
    "\n",
    "        policy = {}\n",
    "        for key in adjusted.keys():\n",
    "            policy[key[1]] = adjusted[key]\n",
    "\n",
    "        return action[1], policy\n",
    "\n",
    "    def getBestMove(self):\n",
    "        selectionState = self.env.toString()\n",
    "        selection = self.__bestAction(selectionState)\n",
    "\n",
    "        movementState = selectionState + \"\".join(str(selection))\n",
    "        moveTo = self.__bestAction(movementState)\n",
    "\n",
    "        shootAtState = movementState + \"\".join(str(moveTo))\n",
    "        shootAt = self.__bestAction(shootAtState)\n",
    "\n",
    "        return selection, moveTo, shootAt\n",
    "\n",
    "    def __bestAction(self, state):\n",
    "        filtered = {key: value for (key, value)\n",
    "                    in self.edgeVisitQuantity.items() if key[0] == state}\n",
    "\n",
    "        action = max(filtered, key=(lambda key: filtered[key]))\n",
    "        return action[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, currentBestNNet=None):\n",
    "        self.CURRENT_BEST_NNET = currentBestNNet\n",
    "        self.device = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "                       else torch.device(\"cpu\"))\n",
    "\n",
    "    def train(self, loops=1, games=10, searchesPerMove=25, numberOfSamples=100):\n",
    "        nnets, optimisers = self.__loadNNets(self.CURRENT_BEST_NNET, True)\n",
    "        for loop in range(loops):\n",
    "            print(f\"Self-play phase:\")\n",
    "            for game in tqdm(range(games)):\n",
    "                env = Environment()\n",
    "                actionsTaken = []\n",
    "                while not env.isGameFinished():\n",
    "                    mcts = MCTS(env, nnets, self.device)\n",
    "                    env.saveCheckpoint()\n",
    "                    for search in range(searchesPerMove):\n",
    "                        mcts.search()\n",
    "                        env.loadCheckpoint()\n",
    "\n",
    "                    nextMove, actions = mcts.getRandomMove()\n",
    "                    env.move(*nextMove)\n",
    "                    actionsTaken += actions\n",
    "\n",
    "                reward = env.getReward()\n",
    "                isBlackWinner = not env.isBlackTurn()\n",
    "\n",
    "                with open(join(dirname(__file__), \"actions.csv\"), \"a\") as file:\n",
    "                    writer = csv.writer(file, delimiter=\"|\")\n",
    "                    for action in actionsTaken:\n",
    "                        wasBlackTurn = action[2]\n",
    "                        action[2] = reward if wasBlackTurn != isBlackWinner else -reward\n",
    "                        writer.writerow(action)\n",
    "\n",
    "            print(\"Weight updating phase\")\n",
    "\n",
    "            actions = pd.read_csv(\n",
    "                join(dirname(__file__), \"actions.csv\"), delimiter=\"|\")\n",
    "\n",
    "            numberOfActions = len(actions.index)\n",
    "            if numberOfActions > 5e6:\n",
    "                actions = actions.tail(5e6)\n",
    "                numberOfActions = 5e6\n",
    "\n",
    "            actions.to_csv(join(dirname(__file__), \"actions.csv\"),\n",
    "                           sep=\"|\", index=False, header=False)\n",
    "\n",
    "            samples, env = [], Environment()\n",
    "            for _ in range(numberOfSamples):\n",
    "                # Randomly sample from DF\n",
    "                sampleIndex = randint(0, numberOfActions-1)\n",
    "                while sampleIndex in samples:\n",
    "                    sampleIndex = randint(0, numberOfActions-1)\n",
    "\n",
    "                samples += [sampleIndex]\n",
    "\n",
    "            for sample in tqdm(samples):\n",
    "                state, policy, value = actions.iloc[sample]\n",
    "                policy = literal_eval(policy)\n",
    "                own, opp, arr, sel, mov = env.parseState(state)\n",
    "\n",
    "                if sel is not None:\n",
    "                    active = np.zeros((10, 10), dtype=np.uint8)\n",
    "                    if mov is None:\n",
    "                        active[sel] = 1\n",
    "                        nnetIndex = 1\n",
    "                    else:\n",
    "                        active[mov] = 1\n",
    "                        own[sel] = 0\n",
    "                        own[mov] = 1\n",
    "\n",
    "                        nnetIndex = 2\n",
    "\n",
    "                    state = (own, opp, arr, active)\n",
    "                else:\n",
    "                    state = (own, opp, arr)\n",
    "                    nnetIndex = 0\n",
    "\n",
    "                state = (torch.tensor(state, dtype=torch.float, device=self.device)\n",
    "                         .unsqueeze(0))\n",
    "\n",
    "                optimisers[nnetIndex].zero_grad()\n",
    "                predictedPolicy, predictedValue = nnets[nnetIndex](state)\n",
    "\n",
    "                policyT = torch.zeros((10, 10),\n",
    "                                      dtype=torch.float, device=self.device)\n",
    "                for action in policy.keys():\n",
    "                    policyT[action] = policy[action]\n",
    "\n",
    "                valueT = torch.tensor(value,\n",
    "                                      dtype=torch.float, device=self.device)\n",
    "\n",
    "                # Cross entropy\n",
    "                xEntropy = -torch.log((1-policyT)-predictedPolicy)\n",
    "                squareErr = (predictedValue - valueT)**2    # Square error\n",
    "\n",
    "                loss = xEntropy + squareErr\n",
    "\n",
    "                loss.mean().backward()\n",
    "                optimisers[nnetIndex].step()\n",
    "\n",
    "        wins, losses = self.__compareToCurrentBest(nnets)\n",
    "        print(\n",
    "            f\"Evaluation results: {wins}W and {losses}L --> {100*(wins/(wins+losses))}%\")\n",
    "\n",
    "        if wins/(wins+losses) >= 0.55:\n",
    "            name = str(datetime.now()) + \".pth\"\n",
    "            print(\"New best network is {name}\")\n",
    "            self.CURRENT_BEST_NNET = name\n",
    "            self.__saveNNets(self, nnets, name)\n",
    "\n",
    "    def __loadNNets(self, name, includeOptimisers=False):\n",
    "        nNetA = NeuralNet(in_channels=3).to(self.device)\n",
    "        nNetB = NeuralNet(in_channels=4).to(self.device)\n",
    "        nNetC = NeuralNet(in_channels=4).to(self.device)\n",
    "\n",
    "        if name is not None:\n",
    "            name.replace(\".pth\", \"a.pth\")\n",
    "            nNetA.load(name)\n",
    "\n",
    "            name.replace(\"a.pth\", \"b.pth\")\n",
    "            nNetB.load(name)\n",
    "\n",
    "            name.replace(\"b.pth\", \"c.pth\")\n",
    "            nNetC.load(name)\n",
    "        else:\n",
    "            nNetA.loadMostRecent(\"a.pth\")\n",
    "            nNetB.loadMostRecent(\"b.pth\")\n",
    "            nNetC.loadMostRecent(\"c.pth\")\n",
    "\n",
    "        nnets = nNetA, nNetB, nNetC\n",
    "        if includeOptimisers:\n",
    "            optimisers = tuple(torch.optim.Adam(\n",
    "                N.parameters(), lr=0.0001) for N in nnets)\n",
    "            return nnets, optimisers\n",
    "        else:\n",
    "            return nnets\n",
    "\n",
    "    def __compareToCurrentBest(self, trainedNets, numberOfGames=10, searchesPerMove=25):\n",
    "        print(\"Evaluating network\")\n",
    "        previousNets = self.__loadNNets(self.CURRENT_BEST_NNET)\n",
    "        wins, losses = 0, 0\n",
    "\n",
    "        for game in tqdm(range(numberOfGames)):\n",
    "            isTrainedBlack = bool(getrandbits(1))\n",
    "            isBlacksMove = False\n",
    "            env = Environment()\n",
    "\n",
    "            while not env.isGameFinished():\n",
    "                if isBlacksMove != isTrainedBlack:\n",
    "                    mcts = MCTS(env, previousNets, self.device)\n",
    "                else:\n",
    "                    mcts = MCTS(env, trainedNets, self.device)\n",
    "\n",
    "                env.saveCheckpoint()\n",
    "                for search in range(searchesPerMove):\n",
    "                    mcts.search()\n",
    "                    env.loadCheckpoint()\n",
    "\n",
    "                nextMove = mcts.getBestMove()\n",
    "                env.move(*nextMove)\n",
    "\n",
    "                isBlacksMove = not isBlacksMove\n",
    "\n",
    "            isBlackWinner = not env.isBlackTurn\n",
    "            if isBlackWinner != isTrainedBlack:\n",
    "                losses += 1\n",
    "            else:\n",
    "                wins += 1\n",
    "\n",
    "        return wins, losses\n",
    "\n",
    "    def __saveNNets(self, nnets, name):\n",
    "        name.replace(\".pth\", \"a.pth\")\n",
    "        nnets[0].save(name)\n",
    "\n",
    "        name.replace(\"a.pth\", \"b.pth\")\n",
    "        nnets[1].save(name)\n",
    "\n",
    "        name.replace(\"b.pth\", \"c.pth\")\n",
    "        nnets[2].save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}