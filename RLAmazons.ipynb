{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "RLAmazons.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d25dad75165e439da0c82b160434adf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1223536e106483db5a7ac60f6ff1f0f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ba209972959d4aa5afccab3071bf490f",
              "IPY_MODEL_e400bf789333499c98a07b1ea7e1ca02"
            ]
          }
        },
        "c1223536e106483db5a7ac60f6ff1f0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba209972959d4aa5afccab3071bf490f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fdbd0d6e12e84776b761ca52852c063d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db4a3e7a4e5740d8bc4fff073fabf197"
          }
        },
        "e400bf789333499c98a07b1ea7e1ca02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9c7cbe7dee4344188dff4443c1307ef0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "  0% 0/10 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb71672695f24933b0cc543b8581e0aa"
          }
        },
        "fdbd0d6e12e84776b761ca52852c063d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db4a3e7a4e5740d8bc4fff073fabf197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c7cbe7dee4344188dff4443c1307ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb71672695f24933b0cc543b8581e0aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y58tmdBn6WQm",
        "colab_type": "text"
      },
      "source": [
        "Reinforcement Learning for the Game of the Amazons\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZR8v6sN6DFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "!pip install -q torch torchvision livelossplot tqdm pandas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZENlfazb6DFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ast import literal_eval\n",
        "from copy import copy\n",
        "import csv\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import randint, getrandbits, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCr7UFyf6DFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Game:\n",
        "    def __init__(self):\n",
        "        # suppress np divide by zero errors for moves\n",
        "        np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "        self.board = np.zeros((3, 10, 10), dtype=np.uint8)\n",
        "        self.isBlackTurn = False\n",
        "        self.__loadStartingBoard()\n",
        "\n",
        "    def __loadStartingBoard(self):\n",
        "        # White amazons\n",
        "        self.board[0, 0, 3] = 1\n",
        "        self.board[0, 3, 0] = 1\n",
        "        self.board[0, 6, 0] = 1\n",
        "        self.board[0, 9, 3] = 1\n",
        "\n",
        "        # Black amazons\n",
        "        self.board[1, 0, 6] = 1\n",
        "        self.board[1, 3, 9] = 1\n",
        "        self.board[1, 6, 9] = 1\n",
        "        self.board[1, 9, 6] = 1\n",
        "\n",
        "    def move(self, fromXY, toXY, shootAT):\n",
        "        unchangedBoard = copy(self.board)\n",
        "\n",
        "        if self.board[int(self.isBlackTurn), fromXY[0], fromXY[1]] == 1:\n",
        "            if self.__isValidMove(fromXY, toXY):\n",
        "                self.board[int(self.isBlackTurn), fromXY[0], fromXY[1]] = 0\n",
        "                self.board[int(self.isBlackTurn), toXY[0], toXY[1]] = 1\n",
        "                if self.__isValidMove(toXY, shootAT):\n",
        "                    self.board[2, shootAT[0], shootAT[1]] = 1\n",
        "                    self.isBlackTurn = not self.isBlackTurn\n",
        "                    return\n",
        "\n",
        "        # if any invalidities\n",
        "        self.board = unchangedBoard\n",
        "        raise Exception(\"That move is invalid.\")\n",
        "\n",
        "    def __isValidMove(self, position, destination):\n",
        "        direction = np.subtract(destination, position)\n",
        "        direction = direction / np.abs(direction)\n",
        "        direction = np.array(np.nan_to_num(direction), dtype=np.int8)\n",
        "\n",
        "        valid = True\n",
        "        checkingPosition = position + direction\n",
        "\n",
        "        while tuple(checkingPosition) != destination:\n",
        "            for i in range(3):\n",
        "                valid &= (\n",
        "                    self.board[i, checkingPosition[0], checkingPosition[1]] == 0)\n",
        "            checkingPosition = checkingPosition + direction\n",
        "\n",
        "        return valid\n",
        "\n",
        "    def isGameFinished(self):\n",
        "        currentPlayersAmazons = np.nonzero(self.board[int(self.isBlackTurn)])\n",
        "        currentPlayersAmazons = np.transpose(currentPlayersAmazons)\n",
        "        canMove = False\n",
        "\n",
        "        # Check if there exists an empty cell neighbouring an amazon\n",
        "        for amazon in currentPlayersAmazons:\n",
        "            for i in range(-1, 2):\n",
        "                for j in range(-1, 2):\n",
        "                    if (i, j) != (0, 0):\n",
        "                        pos = amazon[0] + i, amazon[1] + j\n",
        "                        if self.__isInBoard(pos):\n",
        "                            isPosEmpty = True\n",
        "                            for k in range(3):\n",
        "                                isPosEmpty &= self.board[k,\n",
        "                                                         pos[0], pos[1]] == 0\n",
        "                            if isPosEmpty:\n",
        "                                return False\n",
        "        return True\n",
        "\n",
        "    def __isInBoard(self, pos):\n",
        "        isIn = pos[0] > -1 and pos[0] < 10\n",
        "        isIn &= pos[1] > -1 and pos[1] < 10\n",
        "        return isIn\n",
        "\n",
        "    def calculateReward(self):\n",
        "        winner = int(not self.isBlackTurn)\n",
        "        winnersAmazons = np.nonzero(self.board[winner])\n",
        "        winnersAmazons = np.transpose(winnersAmazons)\n",
        "\n",
        "        rewardCells = np.zeros((10, 10), dtype=np.uint8)\n",
        "        for amazon in winnersAmazons:\n",
        "            rewardCells = self.__rewardHelper(amazon, rewardCells)\n",
        "\n",
        "        return(np.count_nonzero(rewardCells))\n",
        "\n",
        "    def __rewardHelper(self, start, cells):\n",
        "        for i in range(-1, 2):\n",
        "            for j in range(-1, 2):\n",
        "                if (i, j) != (0, 0):\n",
        "                    pos = start[0] + i, start[1] + j\n",
        "                    if self.__isInBoard(pos):\n",
        "                        if cells[pos[0], pos[1]] == 0:\n",
        "                            empty = True\n",
        "                            for k in range(3):\n",
        "                                empty &= self.board[k, pos[0], pos[1]] == 0\n",
        "\n",
        "                            if empty:\n",
        "                                cells[pos[0], pos[1]] = 1\n",
        "                                cells = self.__rewardHelper(pos, cells)\n",
        "        return cells\n",
        "\n",
        "    def rollbackTo(self, board, turn):\n",
        "        self.board = board\n",
        "        self.isBlackTurn = turn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRLFVWks6DFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Environment():\n",
        "    def __init__(self):\n",
        "        self.game = Game()\n",
        "        self.currentCheckpoint = None\n",
        "\n",
        "    def isGameFinished(self):\n",
        "        return self.game.isGameFinished()\n",
        "\n",
        "    def getState(self):\n",
        "        return tuple(self.game.board)\n",
        "\n",
        "    def isBlackTurn(self):\n",
        "        return self.game.isBlackTurn\n",
        "\n",
        "    def getReward(self):\n",
        "        return 1\n",
        "        \n",
        "    def move(self, fromXY, toXY, shotXY):\n",
        "        return self.game.move(fromXY, toXY, shotXY)\n",
        "\n",
        "    def saveCheckpoint(self):\n",
        "        self.currentCheckpoint = {\n",
        "            \"board\": self.game.board,\n",
        "            \"turn\": self.game.isBlackTurn\n",
        "        }\n",
        "\n",
        "    def loadCheckpoint(self):\n",
        "        self.game.board = copy(self.currentCheckpoint[\"board\"])\n",
        "        self.game.isBlackTurn = copy(self.currentCheckpoint[\"turn\"])\n",
        "\n",
        "    def getSelectionMask(self):\n",
        "        return self.game.board[int(self.game.isBlackTurn)]\n",
        "\n",
        "    def getMovementMask(self, moveFrom):\n",
        "        occupiedCells = []\n",
        "        for stateImg in self.game.board:\n",
        "            occupiedCells += list(map(tuple, np.argwhere(stateImg > 0)))\n",
        "        return self.__validityPoller(moveFrom, occupiedCells)\n",
        "\n",
        "    def __validityPoller(self, start, blockers):\n",
        "        valid = np.zeros((10, 10))\n",
        "        for dirX in range(-1, 2):\n",
        "            for dirY in range(-1, 2):\n",
        "                if dirX == 0 and dirY == 0:\n",
        "                    continue    # Goes nowhere\n",
        "\n",
        "                x, y = start\n",
        "                x += dirX\n",
        "                y += dirY\n",
        "\n",
        "                while x > -1 and x < 10 and y > -1 and y < 10:\n",
        "                    if (x, y) in blockers:\n",
        "                        break\n",
        "\n",
        "                    valid[x, y] = 1\n",
        "\n",
        "                    x += dirX\n",
        "                    y += dirY\n",
        "\n",
        "        return valid\n",
        "\n",
        "    def getShotMask(self, newAmazonPos, oldAmazonPos):\n",
        "        occupiedCells = []\n",
        "        for stateImg in self.game.board:\n",
        "            occupiedCells += list(map(tuple, np.argwhere(stateImg > 0)))\n",
        "\n",
        "        if oldAmazonPos in occupiedCells:\n",
        "            occupiedCells.remove(oldAmazonPos)\n",
        "\n",
        "        return self.__validityPoller(newAmazonPos, occupiedCells)\n",
        "\n",
        "    def toString(self, state=None):\n",
        "\n",
        "        if state is None:\n",
        "            state = self.currentCheckpoint[\"board\"]\n",
        "\n",
        "        string = \"\"\n",
        "        for arr in state[:-1]:\n",
        "            for point in np.transpose(np.nonzero(arr)):\n",
        "                string += str(point[0]) + str(point[1])\n",
        "\n",
        "        for char in np.nditer(state[-1]):\n",
        "            string += str(char)\n",
        "\n",
        "        return string\n",
        "\n",
        "    def parseState(self, string):\n",
        "        amazons = []\n",
        "        for i in range(0, 17, 2):\n",
        "            amazons += [(int(string[i]), int(string[i+1]))]\n",
        "\n",
        "        ownAmazons = np.zeros((10, 10), dtype=np.uint8)\n",
        "        for amazon in amazons[:4]:\n",
        "            ownAmazons[amazon] = 1\n",
        "\n",
        "        oppAmazons = np.zeros((10, 10), dtype=np.uint8)\n",
        "        for amazon in amazons[4:]:\n",
        "            oppAmazons[amazon] = 1\n",
        "\n",
        "        arrows = np.fromstring(string[16:116], dtype=np.uint8)\n",
        "        arrows -= ord('0')  # Convert from unicode to binary\n",
        "        arrows = np.reshape(arrows, (10, 10))\n",
        "\n",
        "        selection, movement = None, None\n",
        "\n",
        "        if len(string) > 116:\n",
        "            selection = (int(string[116]), int(string[117]))\n",
        "\n",
        "        if len(string) > 118:\n",
        "            movement = (int(string[118]), int(string[119]))\n",
        "\n",
        "        return ownAmazons, oppAmazons, arrows, selection, movement"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QfR6ido6DFq",
        "colab_type": "code",
        "outputId": "a831960f-47ca-48b0-f350-4c76ae639ba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    NUMBER_OF_RESIDUAL_LAYERS = 40\n",
        "\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.inputLayer = self.__inputLayer(in_channels)\n",
        "        self.residualBlock = self.__residualBlock()\n",
        "        self.policyHead = self.__policyHead()\n",
        "        self.valueHead = self.__valueHead()\n",
        "\n",
        "    def __inputLayer(self, in_channels):\n",
        "        layers = nn.ModuleList()\n",
        "        layers.append(nn.Conv2d(in_channels, out_channels=300,\n",
        "                                kernel_size=3, padding=1, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(300))\n",
        "        layers.append(nn.ReLU())\n",
        "        return layers\n",
        "\n",
        "    def __residualBlock(self):\n",
        "        layers = nn.ModuleList()\n",
        "        layers.append(nn.Conv2d(in_channels=300, out_channels=300,\n",
        "                                kernel_size=3, padding=1, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(300))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Conv2d(in_channels=300, out_channels=300,\n",
        "                                kernel_size=3, padding=1, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(300))\n",
        "        return layers\n",
        "\n",
        "    def __policyHead(self):\n",
        "        layers = nn.ModuleList()\n",
        "        layers.append(nn.Conv2d(in_channels=300, out_channels=1,\n",
        "                                kernel_size=1, padding=0, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(1))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(10, 10))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        return layers\n",
        "\n",
        "    def __valueHead(self):\n",
        "        layers = nn.ModuleList()\n",
        "        layers.append(nn.Conv2d(in_channels=300, out_channels=1,\n",
        "                                kernel_size=1, padding=0, bias=False))\n",
        "        layers.append(nn.BatchNorm2d(1))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(10, 10))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Linear(10, 10))\n",
        "        layers.append(nn.MaxPool2d(10))\n",
        "        layers.append(nn.Tanh())\n",
        "        return layers\n",
        "\n",
        "    def forward(self, networkInput):\n",
        "        for layer in self.inputLayer:\n",
        "            networkInput = layer(networkInput)\n",
        "\n",
        "        for _ in range(self.NUMBER_OF_RESIDUAL_LAYERS):\n",
        "            original = networkInput     # For skip connection\n",
        "            for layer in self.residualBlock:\n",
        "                networkInput = layer(networkInput)\n",
        "            networkInput = torch.relu(networkInput + original)\n",
        "\n",
        "        policy = networkInput\n",
        "        value = networkInput\n",
        "        for layer in self.policyHead:\n",
        "            policy = layer(policy)\n",
        "\n",
        "        for layer in self.valueHead:\n",
        "            value = layer(value)\n",
        "\n",
        "        return policy.view(10, 10), value\n",
        "\n",
        "    def save(self, name):\n",
        "        path = f\"/content/gdrive/My Drive/AmazonsData/Models/{name}\"\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def __loadPath(self, path):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        self.eval()\n",
        "\n",
        "    def load(self, name):\n",
        "        path = f\"/content/gdrive/My Drive/AmazonsData/Models/{name}\"\n",
        "        self.__loadPath(path)\n",
        "\n",
        "    def loadMostRecent(self, typeOfNet):\n",
        "        directory = \"/content/gdrive/My Drive/AmazonsData/Models/\"\n",
        "        allPaths = [join(directory, name)\n",
        "                    for name in listdir(directory) if typeOfNet in name]\n",
        "\n",
        "        if len(allPaths) < 1:\n",
        "            print(\"There are no saved models in the models folder. Starting fresh..\")\n",
        "        else:\n",
        "            self.__loadPath(max(allPaths, key=getctime))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    net = NeuralNet()\n",
        "    net.loadMostRecent(\"a\")\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are no saved models in the models folder. Starting fresh..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umyo-nLk6DFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MCTS():\n",
        "\n",
        "    def __init__(self, env, nets, device):\n",
        "        self.env = env\n",
        "        self.nets = nets\n",
        "        self.device = device\n",
        "\n",
        "        self.qValues = {}\n",
        "        self.policies = {}\n",
        "        self.edgeVisitQuantity = {}\n",
        "        self.nodeVisitQuantity = {}\n",
        "\n",
        "        self.valids = {}\n",
        "\n",
        "    def search(self):\n",
        "        if self.env.isGameFinished():\n",
        "            # Returns 3-tuple reward\n",
        "            return (self.env.getReward(),) * 3\n",
        "\n",
        "        values = [0, 0, 0]\n",
        "\n",
        "        state = self.env.getState()\n",
        "        stateString = self.env.toString(state)\n",
        "        stateTensor = torch.tensor(\n",
        "            state, dtype=torch.float, device=self.device).unsqueeze_(0)\n",
        "\n",
        "        if stateString not in self.policies:\n",
        "            self.policies[stateString], values[0] = self.nets[0](stateTensor)\n",
        "            validSelections = torch.tensor(\n",
        "                self.env.getSelectionMask(), device=self.device)\n",
        "\n",
        "            self.policies[stateString] *= validSelections  # mask out invalids\n",
        "\n",
        "            validCoordinates = np.nonzero(validSelections)\n",
        "            self.valids[stateString] = validCoordinates\n",
        "\n",
        "        validCoordinates = self.valids[stateString]\n",
        "        bestScore = float(\"-inf\")\n",
        "        bestSelection = None\n",
        "        bestSelectionStr = \"\"\n",
        "        bestSelectionArr = None\n",
        "\n",
        "        for selection in validCoordinates:\n",
        "            selection = tuple(coord.item() for coord in selection)\n",
        "            selectionArr = np.zeros((10, 10), dtype=np.uint8)\n",
        "            selectionArr[selection] = 1\n",
        "            selectionTuple = state + (selectionArr,)\n",
        "            selectionString = stateString + \\\n",
        "                str(selection[0]) + str(selection[1])\n",
        "\n",
        "            if selectionString not in self.valids:\n",
        "                self.valids[selectionString] = self.env.getMovementMask(\n",
        "                    selection)\n",
        "\n",
        "            if len(np.transpose(np.nonzero(self.valids[selectionString]))) < 1:\n",
        "                valid = self.valids[stateString]\n",
        "                selectionTensor = torch.tensor(\n",
        "                    selection, device=self.device, dtype=torch.float)\n",
        "\n",
        "                for i in range(len(valid)):\n",
        "                    if torch.all(torch.eq(valid[i],  selectionTensor)):\n",
        "                        valid = torch.cat([valid[:i], valid[i+1:]])\n",
        "                        break\n",
        "\n",
        "                self.valids[stateString] = valid\n",
        "\n",
        "            else:\n",
        "                if (stateString, selection) in self.qValues:\n",
        "                    score = self.qValues[(stateString, selection)] + \\\n",
        "                        self.policies[stateString][selection] * \\\n",
        "                        sqrt(self.nodeVisitQuantity.get(stateString, 0)) / \\\n",
        "                        (self.edgeVisitQuantity.get((stateString, selection), 0) + 1)\n",
        "\n",
        "                else:\n",
        "                    score = self.policies[stateString][selection] * \\\n",
        "                        sqrt(self.nodeVisitQuantity.get(stateString, 0))\n",
        "\n",
        "                if score > bestScore:\n",
        "                    bestScore = score\n",
        "                    bestSelection = selection\n",
        "                    bestSelectionStr = selectionString\n",
        "                    bestSelectionArr = selectionTuple\n",
        "\n",
        "        if bestSelectionStr not in self.policies:\n",
        "            movementTensor = torch.tensor(\n",
        "                bestSelectionArr, dtype=torch.float, device=self.device).unsqueeze(0)\n",
        "            self.policies[bestSelectionStr], values[1] = self.nets[1](\n",
        "                movementTensor)\n",
        "\n",
        "            # mask out invalids\n",
        "            self.policies[bestSelectionStr] *= torch.tensor(\n",
        "                self.valids[bestSelectionStr], device=self.device)\n",
        "\n",
        "        validCoordinates = np.transpose(\n",
        "            np.nonzero(self.valids[bestSelectionStr]))\n",
        "        bestScore = float(\"-inf\")\n",
        "        bestMove = None\n",
        "\n",
        "        # Choose coord to move to\n",
        "        for moveTo in validCoordinates:\n",
        "            moveTo = tuple(coord.item() for coord in moveTo)\n",
        "\n",
        "            if (bestSelectionStr, moveTo) in self.qValues:\n",
        "                score = self.qValues[(bestSelectionStr, moveTo)] + \\\n",
        "                    self.policies[bestSelectionStr][moveTo] * \\\n",
        "                    sqrt(self.nodeVisitQuantity.get(bestSelectionStr, 0)) / \\\n",
        "                    (self.edgeVisitQuantity.get((bestSelectionStr, moveTo), 0) + 1)\n",
        "            else:\n",
        "                score = self.policies[bestSelectionStr][moveTo] * \\\n",
        "                    sqrt(self.nodeVisitQuantity.get(bestSelectionStr, 0))\n",
        "\n",
        "            if score > bestScore:\n",
        "                bestScore = score\n",
        "                bestMove = moveTo\n",
        "\n",
        "        bestMoveString = bestSelectionStr + str(bestMove[0]) + str(bestMove[1])\n",
        "        shotTensor = torch.tensor(\n",
        "            bestSelectionArr, dtype=torch.float, device=self.device)\n",
        "        shotTensor[0][bestSelection], shotTensor[0][bestMove] = 0, 1\n",
        "        shotTensor[3][bestSelection], shotTensor[3][bestMove] = 0, 1\n",
        "        shotTensor.unsqueeze_(0)\n",
        "\n",
        "        # New leaf node\n",
        "        if bestMoveString not in self.policies:\n",
        "            self.policies[bestMoveString], values[2] = self.nets[2](shotTensor)\n",
        "            validSelections = torch.tensor(\n",
        "                self.env.getShotMask(bestMove, bestSelection), device=self.device)\n",
        "\n",
        "            # mask out invalids\n",
        "            self.policies[bestMoveString] *= validSelections\n",
        "\n",
        "            validCoordinates = np.nonzero(validSelections)\n",
        "            self.valids[bestMoveString] = validCoordinates\n",
        "            return [-value for value in values]\n",
        "\n",
        "        validShots = self.valids[bestMoveString]\n",
        "        bestScore = float(\"-inf\")\n",
        "        bestShot = None\n",
        "\n",
        "        for shot in validShots:\n",
        "            shot = tuple(coord.item() for coord in shot)\n",
        "            if (bestMoveString, shot) in self.qValues:\n",
        "                score = self.qValues[(bestMoveString, shot)] + \\\n",
        "                    self.policies[bestMoveString][shot] * \\\n",
        "                    sqrt(self.nodeVisitQuantity.get(bestMoveString, 0)) / \\\n",
        "                    (self.edgeVisitQuantity.get((bestMoveString, shot), 0) + 1)\n",
        "            else:\n",
        "                score = self.policies[bestMoveString][shot] * \\\n",
        "                    sqrt(self.nodeVisitQuantity.get(bestMoveString, 0))\n",
        "\n",
        "            if score > bestScore:\n",
        "                bestScore = score\n",
        "                bestShot = shot\n",
        "\n",
        "        self.env.move(bestSelection, bestMove, bestShot)\n",
        "\n",
        "        values = self.search()\n",
        "        pairs = ((stateString, bestSelection),\n",
        "                 (bestSelectionStr, bestMove),\n",
        "                 (bestMoveString, bestShot))\n",
        "\n",
        "        for i in range(3):\n",
        "            if pairs[i] in self.qValues:\n",
        "                self.qValues[pairs[i]] = (\n",
        "                    self.qValues[pairs[i]] *\n",
        "                    self.edgeVisitQuantity[pairs[i]] +\n",
        "                    values[i] /\n",
        "                    (self.edgeVisitQuantity[pairs[i]] + 1))\n",
        "\n",
        "                self.edgeVisitQuantity[pairs[i]] += 1\n",
        "\n",
        "            else:\n",
        "                x, y = pairs[i][1]\n",
        "                self.qValues[pairs[i]] = values[i]\n",
        "                self.edgeVisitQuantity[pairs[i]] = 1\n",
        "\n",
        "            if pairs[i][0] in self.nodeVisitQuantity:\n",
        "                self.nodeVisitQuantity[pairs[i][0]] += 1\n",
        "            else:\n",
        "                self.nodeVisitQuantity[pairs[i][0]] = 1\n",
        "\n",
        "        return [-value for value in values]\n",
        "\n",
        "    def getRandomMove(self):\n",
        "        selectionState = self.env.toString()\n",
        "        selection, selPolicy = self.__weightedRandomAction(selectionState)\n",
        "\n",
        "        movementState = selectionState + str(selection[0]) + str(selection[1])\n",
        "        moveTo, movePolicy = self.__weightedRandomAction(movementState)\n",
        "\n",
        "        shootAtState = movementState + str(moveTo[0]) + str(moveTo[1])\n",
        "        shootAt, shotPolicy = self.__weightedRandomAction(shootAtState)\n",
        "\n",
        "        return (selection, moveTo, shootAt), [[selectionState, selPolicy, self.env.isBlackTurn()],\n",
        "                                              [movementState, movePolicy,\n",
        "                                               self.env.isBlackTurn()],\n",
        "                                              [shootAtState, shotPolicy, self.env.isBlackTurn()]]\n",
        "\n",
        "    def __weightedRandomAction(self, state):\n",
        "        filtered = {key: value for (key, value)\n",
        "                    in self.edgeVisitQuantity.items() if key[0] == state}\n",
        "\n",
        "        total = sum(filtered.values())\n",
        "        adjusted = {key: value / total\n",
        "                    for (key, value) in filtered.items()}\n",
        "\n",
        "        randomChoice, total, action = random(), 0, None\n",
        "        for key, value in adjusted.items():\n",
        "            total += value\n",
        "            if randomChoice <= total:\n",
        "                action = key\n",
        "                break\n",
        "\n",
        "        policy = {}\n",
        "        for key in adjusted.keys():\n",
        "            policy[key[1]] = adjusted[key]\n",
        "\n",
        "        return action[1], policy\n",
        "\n",
        "    def getBestMove(self):\n",
        "        selectionState = self.env.toString()\n",
        "        selection = self.__bestAction(selectionState)\n",
        "\n",
        "        movementState = selectionState + str(selection[0]) + str(selection[1])\n",
        "        moveTo = self.__bestAction(movementState)\n",
        "\n",
        "        shootAtState = movementState + str(moveTo[0]) + str(moveTo[1])\n",
        "        shootAt = self.__bestAction(shootAtState)\n",
        "\n",
        "        return selection, moveTo, shootAt\n",
        "\n",
        "    def __bestAction(self, state):\n",
        "        filtered = {key: value for (key, value)\n",
        "                    in self.edgeVisitQuantity.items() if key[0] == state}\n",
        "\n",
        "        action = max(filtered, key=(lambda key: filtered[key]))\n",
        "        return action[1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFXXa2766DFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, currentBestNNet=None):\n",
        "        self.CURRENT_BEST_NNET = currentBestNNet\n",
        "        self.device = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
        "                       else torch.device(\"cpu\"))\n",
        "\n",
        "    def train(self, loops=1, games=0, searchesPerMove=25, numberOfSamples=2048):\n",
        "        nnets, optimisers = self.__loadNNets(self.CURRENT_BEST_NNET, True)\n",
        "        for loop in range(loops):\n",
        "            print(f\"Self-play phase:\")\n",
        "            for game in tqdm(range(games)):\n",
        "                env = Environment()\n",
        "                actionsTaken = []\n",
        "                while not env.isGameFinished():\n",
        "                    mcts = MCTS(env, nnets, self.device)\n",
        "                    env.saveCheckpoint()\n",
        "                    for search in range(searchesPerMove):\n",
        "                        mcts.search()\n",
        "                        env.loadCheckpoint()\n",
        "\n",
        "                    nextMove, actions = mcts.getRandomMove()\n",
        "                    env.move(*nextMove)\n",
        "                    actionsTaken += actions\n",
        "\n",
        "                reward = env.getReward()\n",
        "                isBlackWinner = not env.isBlackTurn()\n",
        "\n",
        "                with open(\"/content/gdrive/My Drive/AmazonsData/actions.csv\", \"a\") as file:\n",
        "                    writer = csv.writer(file, delimiter=\"|\")\n",
        "                    for action in actionsTaken:\n",
        "                        wasBlackTurn = action[2]\n",
        "                        action[2] = reward if wasBlackTurn != isBlackWinner else -reward\n",
        "                        writer.writerow(action)\n",
        "\n",
        "            print(\"Weight updating phase\")\n",
        "\n",
        "            actions = pd.read_csv(\"/content/gdrive/My Drive/AmazonsData/actions.csv\", delimiter=\"|\")\n",
        "\n",
        "            numberOfActions = len(actions.index)\n",
        "            if numberOfActions > 5e6:\n",
        "                actions = actions.tail(5e6)\n",
        "                numberOfActions = 5e6\n",
        "\n",
        "            actions.to_csv(\"/content/gdrive/My Drive/AmazonsData/actions.csv\",\n",
        "                           sep=\"|\", index=False, header=False)\n",
        "\n",
        "            samples, env = [], Environment()\n",
        "            for _ in range(numberOfSamples):\n",
        "                # Randomly sample from DF\n",
        "                sampleIndex = randint(0, numberOfActions-1)\n",
        "                while sampleIndex in samples:\n",
        "                    sampleIndex = randint(0, numberOfActions-1)\n",
        "\n",
        "                samples += [sampleIndex]\n",
        "\n",
        "            for sample in tqdm(samples):\n",
        "                state, policy, value = actions.iloc[sample]\n",
        "                policy = literal_eval(policy)\n",
        "                own, opp, arr, sel, mov = env.parseState(state)\n",
        "\n",
        "                if sel is not None:\n",
        "                    active = np.zeros((10, 10), dtype=np.uint8)\n",
        "                    if mov is None:\n",
        "                        active[sel] = 1\n",
        "                        nnetIndex = 1\n",
        "                    else:\n",
        "                        active[mov] = 1\n",
        "                        own[sel] = 0\n",
        "                        own[mov] = 1\n",
        "\n",
        "                        nnetIndex = 2\n",
        "\n",
        "                    state = (own, opp, arr, active)\n",
        "                else:\n",
        "                    state = (own, opp, arr)\n",
        "                    nnetIndex = 0\n",
        "\n",
        "                state = (torch.tensor(state, dtype=torch.float, device=self.device)\n",
        "                         .unsqueeze(0))\n",
        "\n",
        "                optimisers[nnetIndex].zero_grad()\n",
        "                predictedPolicy, predictedValue = nnets[nnetIndex](state)\n",
        "\n",
        "                policyT = torch.zeros((10, 10),\n",
        "                                      dtype=torch.float, device=self.device)\n",
        "                for action in policy.keys():\n",
        "                    policyT[action] = policy[action]\n",
        "\n",
        "                valueT = torch.tensor(value,\n",
        "                                      dtype=torch.float, device=self.device)\n",
        "\n",
        "                # Cross entropy\n",
        "                xEntropy = -torch.log((1-policyT)-predictedPolicy)\n",
        "                squareErr = (predictedValue - valueT)**2    # Square error\n",
        "\n",
        "                loss = xEntropy + squareErr\n",
        "\n",
        "                loss.mean().backward()\n",
        "                optimisers[nnetIndex].step()\n",
        "\n",
        "        wins, losses = self.__compareToCurrentBest(nnets)\n",
        "        print(\n",
        "            f\"Evaluation results: {wins}W and {losses}L --> {100*(wins/(wins+losses))}%\")\n",
        "\n",
        "        if wins/(wins+losses) >= 0.55:\n",
        "            name = str(datetime.now()) + \".pth\"\n",
        "            print(\"New best network is {name}\")\n",
        "            self.CURRENT_BEST_NNET = name\n",
        "            self.__saveNNets(self, nnets, name)\n",
        "\n",
        "    def __loadNNets(self, name, includeOptimisers=False):\n",
        "        nNetA = NeuralNet(in_channels=3).to(self.device)\n",
        "        nNetB = NeuralNet(in_channels=4).to(self.device)\n",
        "        nNetC = NeuralNet(in_channels=4).to(self.device)\n",
        "\n",
        "        if name is not None:\n",
        "            name.replace(\".pth\", \"a.pth\")\n",
        "            nNetA.load(name)\n",
        "\n",
        "            name.replace(\"a.pth\", \"b.pth\")\n",
        "            nNetB.load(name)\n",
        "\n",
        "            name.replace(\"b.pth\", \"c.pth\")\n",
        "            nNetC.load(name)\n",
        "        else:\n",
        "            nNetA.loadMostRecent(\"a.pth\")\n",
        "            nNetB.loadMostRecent(\"b.pth\")\n",
        "            nNetC.loadMostRecent(\"c.pth\")\n",
        "\n",
        "        nnets = nNetA, nNetB, nNetC\n",
        "        if includeOptimisers:\n",
        "            optimisers = tuple(torch.optim.Adam(\n",
        "                N.parameters(), lr=0.0001) for N in nnets)\n",
        "            return nnets, optimisers\n",
        "        else:\n",
        "            return nnets\n",
        "\n",
        "    def __compareToCurrentBest(self, trainedNets, numberOfGames=10, searchesPerMove=25):\n",
        "        print(\"Evaluating network\")\n",
        "        previousNets = self.__loadNNets(self.CURRENT_BEST_NNET)\n",
        "        wins, losses = 0, 0\n",
        "\n",
        "        for game in tqdm(range(numberOfGames)):\n",
        "            isTrainedBlack = bool(getrandbits(1))\n",
        "            isBlacksMove = False\n",
        "            env = Environment()\n",
        "\n",
        "            while not env.isGameFinished():\n",
        "                if isBlacksMove != isTrainedBlack:\n",
        "                    mcts = MCTS(env, previousNets, self.device)\n",
        "                else:\n",
        "                    mcts = MCTS(env, trainedNets, self.device)\n",
        "\n",
        "                env.saveCheckpoint()\n",
        "                for search in range(searchesPerMove):\n",
        "                    mcts.search()\n",
        "                    env.loadCheckpoint()\n",
        "\n",
        "                nextMove = mcts.getBestMove()\n",
        "                env.move(*nextMove)\n",
        "\n",
        "                isBlacksMove = not isBlacksMove\n",
        "\n",
        "            isBlackWinner = not env.isBlackTurn\n",
        "            if isBlackWinner != isTrainedBlack:\n",
        "                losses += 1\n",
        "            else:\n",
        "                wins += 1\n",
        "\n",
        "        return wins, losses\n",
        "\n",
        "    def __saveNNets(self, nnets, name):\n",
        "        name.replace(\".pth\", \"a.pth\")\n",
        "        nnets[0].save(name)\n",
        "\n",
        "        name.replace(\"a.pth\", \"b.pth\")\n",
        "        nnets[1].save(name)\n",
        "\n",
        "        name.replace(\"b.pth\", \"c.pth\")\n",
        "        nnets[2].save(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkwiQD829KjD",
        "colab_type": "text"
      },
      "source": [
        "Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGsCM80S6DF1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "d25dad75165e439da0c82b160434adf7",
            "c1223536e106483db5a7ac60f6ff1f0f",
            "ba209972959d4aa5afccab3071bf490f",
            "e400bf789333499c98a07b1ea7e1ca02",
            "fdbd0d6e12e84776b761ca52852c063d",
            "db4a3e7a4e5740d8bc4fff073fabf197",
            "9c7cbe7dee4344188dff4443c1307ef0",
            "cb71672695f24933b0cc543b8581e0aa"
          ]
        },
        "outputId": "fe6ed2d9-f292-4daf-d159-ad52bb0285ee"
      },
      "source": [
        "Agent().train(loops=5, games=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are no saved models in the models folder. Starting fresh..\n",
            "There are no saved models in the models folder. Starting fresh..\n",
            "There are no saved models in the models folder. Starting fresh..\n",
            "Self-play phase:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d25dad75165e439da0c82b160434adf7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}